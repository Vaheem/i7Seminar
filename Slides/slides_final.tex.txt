% This file provides an example Beamer presentation using the RWTH theme
% showcasing some of the more common options, similar to the Powerpoint version
% 12.11.2014: Revision 1 (Harold Bruintjes, Tim Lange)

% For RWTH, beamer should be loaded with class option t (top)
\documentclass[aspectratio=169]{beamer}
\usepackage{tikz}
\usepackage{lmodern}%
% Use fontspec to get Arial font
% Requires use of XeLaTeX
%\usepackage{fontspec}
%\setmainfont{Arial}
%\setsansfont{Arial}
% Also force Arial for math for a more consistent look
%\usepackage{unicode-math}
%\setmathfont{Arial}

\usepackage{helvet}
\renewcommand{\familydefault}{\sfdefault}
\usepackage{MnSymbol}%
\usepackage[utf8]{inputenc} % Allows inpit in utf8
% German-style date formatting (footer)
\usepackage[ddmmyyyy]{datetime}

\usepackage{booktabs} % Allows the use of \toprule, \midrule and \bottomrule in tables

\renewcommand{\dateseparator}{.}

% Format the captions used for figures etc.
\usepackage[compatibility=false]{caption}
\captionsetup{singlelinecheck=off,justification=raggedleft,labelformat=empty,labelsep=none}

% PGFPlots is used for drawing some of the charts
\usepackage{pgfplots}
\pgfplotsset{compat=newest}
\input{plot_commands.tex}

% Load the actual RWTH theme. Suggested is to load the full theme,
% as it requires some specific dimensions
\usetheme{rwth}
\graphicspath{{../figures/}}

% Setup presentation information
\author[First Author]{Vahe Eminyan}
\institute[RWTH]{RWTH Aachen University}
\title[]{Latent Semantic Indexing}
%\title{Seminar “Theoretical Topics in Data Science”}


% Set the logo to the file `logo`
% It will be scaled automatically
%\logo{\includegraphics{rwth_i5_en_rgb}}

% Uncomment this if you want a TOC at every section start
%\AtBeginSection{\frame{
%    \frametitle{Content}
%    \tableofcontents[currentsection]
%}}

% Use this to control some aspects of the footer
%\setbeamertemplate{footertextextra}{}
%\setbeamertemplate{footertext}{Example of an overridden footer}

\definecolor{Mahogany}{rgb}{0.75, 0.25, 0.0}
\usepackage{xcolor}


\begin{document}

\setbeamercolor{title page bar}{fg=rwth}
\setbeamertemplate{title page}[rwth]{}
\begin{frame}[plain]
\titlepage % Print the title page as the first slide
\parbox{0cm}{
  \vspace{-5em}
  \begin{tabbing}
      \=\textbf{Seminar “Theoretical Topics in Data Science”}
  \end{tabbing}
  \vspace{0em}
  \begin{tabbing}
  
    \=\textbf{Vahe~Eminyan}\\
    \=\Large{vahe.eminyan@rwth-aachen.de}\\
    \=\Large{\today}\\[0.5em]
  \end{tabbing}
}
\end{frame}

\begin{frame}
\frametitle{Overview} % Table of contents slide, comment this block out to remove it
\tableofcontents % Throughout your presentation, if you choose to use \section{} and \subsection{} commands, these will automatically be printed on this slide as an overview of your presentation
\end{frame}

%----------------------------------------------------------------------------------------
%	PRESENTATION SLIDES
%----------------------------------------------------------------------------------------

%------------------------------------------------
\section{Introduction} % Sections can be created in order to organize your presentation into discrete blocks, all sections and subsections are automatically printed in the table of contents as an overview of the talk
%------------------------------------------------

%\subsection{Motivation}
%Here should be 

\begin{frame}{Motivation}
\begin{itemize}
    \item Large datasets, often organized in tabular form, represented as \color{Mahogany}matrices \color{black}
    \begin{itemize}
        \item Term-document matrix representing word occurrence in documents
        \item Movie-user matrix representing watched movies of users
    \end{itemize}
    %\item Each entry defined with frequency based function
    \item Interesting aspects
    \begin{itemize}
        \item \color{Mahogany}Find \color{black} documents semantically associated with a \color{Mahogany} query \color{black}
        \item \color{Mahogany} Recommend \color{black} a new movie to a user
    \end{itemize}
\end{itemize}
\vspace{1cm}

\hspace*{2cm}
\begin{tabular}{|c|c|c|c|c|}
\hline
 & Doc 1 & Doc 2 & ... &  Doc m \\ \hline
Term 1 & 0 & 1& ... & 1 \\
Term 2 & 1 & 0& ... & 1 \\
... & ... & ... & ... &  ... \\ 
Term n & 1 & 0& ... & 0 \\\hline
\end{tabular}
\hspace*{2cm}
\begin{tikzpicture}
\draw[->, line width=2pt, >=stealth] (0,0) -- (2,0);
\end{tikzpicture}
\hspace*{2cm}
%
$
\begin{array}{c@{}c}
& \text{\color{Mahogany}Documents }  \\
\text{\color{Mahogany}Terms } & \begin{pmatrix}
0 & 1 & \dots & 1 \\
1 & 0 & \dots & 1 \\
\vdots & \vdots & \ddots & \vdots \\
1 & 0 & \dots & 0
\end{pmatrix} \\
& \text{$n \times m$}
\end{array}
$

\end{frame}



\begin{frame}{Latent Semantic Indexing}

\begin{itemize}
    \item \color{Mahogany}  LSI \color{black} as an information retrieval method
    \item Finds the \color{Mahogany} latent (hidden) semantic structure \color{black} of textual data 
    \item Represent term-document matrix as  \color{Mahogany} product of three matrices:  \color{black} term-topic, topic-topic and topic-document matrix
    
    \item Answer queries with help of these matrices
    \item Based on \color{Mahogany} singular value decomposition \color{black} of the matrix
    
    
\end{itemize}
    
\end{frame}




\section{LSI Background}

\begin{frame}{Singular Value Decomposition (SVD) \cite{strang2005}}
\begin{itemize}
    \item Any $n$ by $m$ matrix can be factored into
    \[
        A_{ n \times m} = U_{[n \times r]}D_{[r \times r]}(V_{[m \times r]})^T = \text{ (orthogonal)(diagonal)(orthogonal).}
    \]
    \item $U$: \color{Mahogany}left singular vectors \color{black} ($n$ terms and $r$ topics)
    \item $V$: \color{Mahogany} right singular vectors \color{black} ($m$ documents and $r$ topics)
    \item $D$: \color{Mahogany} Singular values \color{black} $\sigma_1 \geq \sigma_2 \geq ... \geq \sigma_r$ in decreasing order
    ($r \times r$ diagonal matrix representing the "importance" of each topic, where $r$ rank of matrix $A$)
    \item Vector notation $$A = UDV^T = \sum_{i=1}^r \sigma_i u_i v_i^t$$
\end{itemize}

    
\end{frame}

\begin{frame}{Singular Value Decomposition (SVD) Example: Matrix $A$ with rank $r=3$}
\[
\begin{array}{c@{}c}
& \text{\color{Mahogany}Documents }  \\
\text{\color{Mahogany}Terms } & \begin{pmatrix}
1 & 0 & 1 \\
1 & 1 & 0 \\
0 & 1 & 0 \\
0 & 1 & 1 \\
\end{pmatrix} \\
& \text{\color{Mahogany}$A$}
\end{array}
=
\begin{array}{c@{}c}
& \text{\color{Mahogany}Term-Topic similarity }  \\
 & \begin{pmatrix}
-0.48 & -0.79 & -0.11\cdot10^{-14} \\
-0.58 & 0.16 & 0.71  \\
\textbf{-0.34} & \textbf{0.56} & 0.42\cdot10^{-15}  \\
-0.56 & 0.16 & -0.71  \\
\end{pmatrix} \\
& \text{\color{Mahogany}$U$}
\end{array}
\times
\begin{array}{c@{}c}
& \text{\color{Mahogany}Topic "importance" }  \\
 & \begin{pmatrix}
2.1 & 0 & 0 \\
0 & 1.26 & 0 \\
0 & 0 & 1 \\
\end{pmatrix} \\
& \text{\color{Mahogany}$D$}\\
\end{array}

\hspace{15cm}
\times
\begin{array}{c@{}c}
& \text{\color{Mahogany}Topic-Document similarity }  \\
& \begin{pmatrix}
-0.5 & \textbf{-0.71} & -0.5 \\
-0.5 & \textbf{0.71} & -0.5 \\
0.71 & 0.67 \cdot 10^{-15} & -0.711 \\
\end{pmatrix} \\
& \text{\color{Mahogany}$V^T$}
\end{array}
\]
\end{frame}


\begin{frame}{Latent Semantic Indexing based on SVD}
\begin{itemize}
    \item LSI considers $A_k$ the rank $k$ approximation of $A$ (I.e. keep only $k$ most relevant topics)
    \item In the example $k=2$
    \item Map a query to $k$ dimensional space with $U_k$ and then apply cosine similarity to find similar documents in $D_kV_k^T$
\end{itemize}


\[
\begin{array}{c@{}c}
& \text{\color{Mahogany}Documents }  \\
\text{\color{Mahogany}Terms } & \begin{pmatrix}
1.0 & 0.01 & 1 \\
0.51 & 1.01 & 0.51 \\
0.0 & 1.01 & 0.0 \\
0.49 & 0.98 & 0.49 \\
\end{pmatrix} \\
& \text{\color{Mahogany}$A_k$}
\end{array}
=
\begin{array}{c@{}c}
& \text{\color{Mahogany}Term-Topic similarity }  \\
 & \begin{pmatrix}
-0.48 & -0.79  \\
-0.58 & 0.16  \\
\textbf{-0.34} & \textbf{0.56}   \\
-0.56 & 0.16  \\
\end{pmatrix} \\
& \text{\color{Mahogany}$U_k$}
\end{array}
\times
\begin{array}{c@{}c}
& \text{\color{Mahogany}Topic "importance" }  \\
 & \begin{pmatrix}
2.1 & 0  \\
0 & 1.26  \\
\end{pmatrix} \\
& \text{\color{Mahogany}$D_k$}\\
\end{array}
\times
\begin{array}{c@{}c}
& \text{\color{Mahogany}Topic-Document similarity }  \\
& \begin{pmatrix}
-0.5 & \textbf{-0.71} & -0.5 \\
-0.5 & \textbf{0.71} & -0.5 \\
\end{pmatrix} \\
& \text{\color{Mahogany}$V_k^T$}
\end{array}
\]
\end{frame}


\begin{frame}{Latent Semantic Indexing based on SVD}
\begin{theorem} [Eckart and Young \cite{EckartYoung} ]
	Among all $n \times m$ matrices C of rank at most $k$, $A_k$ is the one that minimizes $||A - C||_F^2 = \sum_{i,j}(A_{ij} - C_{ij})^2$, where $F$ denotes the Frobenius norm of a matrix.
\end{theorem}


\[
\begin{array}{c@{}c}
& \text{\color{Mahogany}Documents }  \\
\text{\color{Mahogany}Terms } & \begin{pmatrix}
1.0 & 0.01 & 1 \\
0.51 & 1.01 & 0.51 \\
0.0 & 1.01 & 0.0 \\
0.49 & 0.98 & 0.49 \\
\end{pmatrix} \\
& \text{\color{Mahogany}$A_k$}
\end{array}
\approx
\begin{array}{c@{}c}
& \text{\color{Mahogany}Documents }  \\
\text{\color{Mahogany}Terms } & \begin{pmatrix}
1 & 0 & 1 \\
1 & 1 & 0 \\
0 & 1 & 0 \\
0 & 1 & 1 \\
\end{pmatrix} \\
& \text{\color{Mahogany}$A$}
\end{array}
\]
\end{frame}




\section{Original Paper Overview and Emphasized Aspect}

\begin{frame}{}
\begin{itemize}
    \item LSI has shown strong empirical results
    \item Two important aspects
    \begin{itemize}
        \item Why does LSI find \color{Mahogany} semantically related \color{black}documents?
        \item How can we \color{Mahogany} reduce the computational time \color{black}?
    \end{itemize}
    \item Papadimitriou et al. \cite{APAPADIMITRIOU2000217} investigated both aspects:
    \begin{enumerate}
        \item Under certain constraints on the term-document matrix, semantically related documents are mapped to \color{Mahogany} similar vectors \color{black}
        \item Instead of LSI use \color{Mahogany} LSI by random projection.\color{black}  This reduces the computational time: 
        \begin{itemize}
            \item Map the original term-document matrix into a lower dimensional space
            \item Use LSI on the lower dimensional matrix
        \end{itemize}
        
    \end{enumerate}
    \item In this presentation we focus on the \color{Mahogany} second \color{black} aspect
\end{itemize}
   
\end{frame}


%\section{Brief Mention of Analysis of LSI’s Good Performance}

\section{LSI by Random Projection}
\begin{frame}{}
 \begin{itemize}
    \item In this section we will investigate the question "How we can speed up the computation": Informal formulation of the main theorem of this section (Theorem 5 original paper)
    \item Introduction of theorems and lemmas that are necessary for the proof of the main theorem 
    \item Introduction: the main theorem (Theorem 5 original paper)
    \item Proof of the main theorem (Theorem 5 original paper)
    \item Computational savings achieved by LSI by random projection
\end{itemize}   
\end{frame}


\begin{frame}{Random Projection for Dimensionality Reduction}
Given a matrix $A \in \mathbb{R}^{n \times m}$ and a matrix $R \in \mathbb{R}^{\ell \times n}$. Use matrix $R$ to represent the matrix $A$ in lower dimensional space by preserving pairwise distances between any two points:
$$B = \sqrt{\frac{n}{\ell}} \cdot R^T A \in \mathbb{R}^{\ell \times m}$$

\begin{lemma}[Johnson and Lindenstrauss \cite{johnson1984Lindenstrauss} ]
Let $v \in  \mathbb{R}^{n}$ be a unit vector, let $H$ be a random $\ell$-dimensional subspace through the origin, and let the random variable $X$ denote the square of the length of the projection of $v$ onto $H$. Suppose $0 <\epsilon < 0.5$, and $24 \log n < 1 < \sqrt{n}$. Then, $E[X] = \frac{\ell}{n}$, and
$$Pr(|X -  \frac{\ell}{n}| > \epsilon \frac{\ell}{n}) < 2 \sqrt{\ell} e ^{-(\ell - 1) \epsilon^2 / 4 }$$
\end{lemma}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Two Step LSI}
\begin{enumerate}
    \item Apply a random projection onto $\ell$ dimensions, where $\ell$ is a small value greater than $k$, on $A$.

\begin{align*}
    B = 
\sqrt{\dfrac{n}{\ell}}
\cdot
\begin{pmatrix}
    \phantom{-} \big\lvert \phantom{-} & \phantom{-} \vert \phantom{-} & & \phantom{-} \vert \phantom{-} \\
    r_1 & r_2 & \cdots & r_{\ell} \\
    \phantom{-} \vert \phantom{-} & \phantom{-} \vert \phantom{-} & & \phantom{-} \vert \phantom{-} \\
\end{pmatrix}^T
\cdot
A
%\small 
%\begin{pmatrix}
%    \phantom{-} \big\lvert \phantom{-} & \phantom{-} \vert \phantom{-} & & \phantom{-} \vert \phantom{-} \\
%    a_1 & a_2 & \cdots & a_m \\
%    \phantom{-} \vert \phantom{-} & \phantom{-} \vert \phantom{-} & & \phantom{-} \vert \phantom{-} \\
%\end{pmatrix}
\label{eq:matrixB}
\end{align*}
    
    \item  Apply rank $O(k)$ LSI (because of the random projection, the number of
singular values kept may have to be slightly increased).
\end{enumerate}
\vspace{1cm}
This leads to an improved running time while preventing the expressiveness of the original matrix.
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Background}
    
\end{frame}



%\subsection{Subsection Example} % A subsection can be created just before a set of slides with a common theme to further break down your presentation into chunks

%
%\begin{frame}
%\frametitle{Paragraphs of Text}
%Sed iaculis dapibus gravida. Morbi sed tortor erat, nec interdum arcu. Sed id lorem lectus. Quisque viverra augue id sem ornare non aliquam nibh tristique. Aenean in ligula nisl. Nulla sed tellus ipsum. Donec vestibulum ligula non lorem vulputate fermentum accumsan neque mollis.\\~\\

%Sed diam enim, sagittis nec condimentum sit amet, ullamcorper sit amet libero. Aliquam vel dui orci, a porta odio. Nullam id suscipit ipsum. Aenean lobortis commodo sem, ut commodo leo gravida vitae. Pellentesque vehicula ante iaculis arcu pretium rutrum eget sit amet purus. Integer ornare nulla quis neque ultrices lobortis. Vestibulum ultrices tincidunt libero, quis commodo erat ullamcorper id.
%\end{frame}

%------------------------------------------------
%
%\begin{frame}
%\frametitle{Bullet Points}
%\begin{itemize}
%\item Lorem ipsum dolor sit amet, consectetur adipiscing elit
%\item Aliquam blandit faucibus nisi, sit amet dapibus enim tempus eu
%\item Nulla commodo, erat quis gravida posuere, elit lacus lobortis est, quis porttitor odio mauris at libero
%\item Nam cursus est eget velit posuere pellentesque
%\item Vestibulum faucibus velit a augue condimentum quis convallis nulla gravida
%\end{itemize}
%\end{frame}

%------------------------------------------------

%\begin{frame}
%\frametitle{Blocks of Highlighted Text}
%\begin{block}{Block 1}
%Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer lectus nisl, ultricies in feugiat rutrum, porttitor sit amet augue. Aliquam ut tortor mauris. Sed volutpat ante purus, quis accumsan dolor.
%\end{block}

%\begin{block}{Block 2}
%Pellentesque sed tellus purus. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Vestibulum quis magna at risus dictum tempor eu vitae velit.
%\end{block}

%\begin{block}{Block 3}
%Suspendisse tincidunt sagittis gravida. Curabitur condimentum, enim sed venenatis rutrum, ipsum neque consectetur orci, sed blandit justo nisi ac lacus.
%\end{block}
%\end{frame}

%------------------------------------------------

%\begin{frame}
%\frametitle{Multiple Columns}
%\begin{columns}[c] % The "c" option specifies centered vertical alignment while the "t" option is used for top vertical alignment

%\column{.45\textwidth} % Left column and width
%\textbf{Heading}
%\begin{enumerate}
%\item Statement
%\item Explanation
%\item Example
%\end{enumerate}

%\column{.5\textwidth} % Right column and width
%Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer lectus nisl, ultricies in feugiat rutrum, porttitor sit amet augue. Aliquam ut tortor mauris. Sed volutpat ante purus, quis accumsan dolor.

%\end{columns}
%\end{frame}

%------------------------------------------------
%\section{Second Section}
%------------------------------------------------

%\begin{frame}
%\frametitle{Table}
%\begin{table}
%\begin{tabular}{l l l}
%\toprule
%\textbf{Treatments} & \textbf{Response 1} & \textbf{Response 2}\\
%\midrule
%Treatment 1 & 0.0003262 & 0.562 \\
%Treatment 2 & 0.0015681 & 0.910 \\
%Treatment 3 & 0.0009271 & 0.296 \\
%\bottomrule
%\end{tabular}
%\caption{Table caption}
%\end{table}
%\end{frame}

%------------------------------------------------



%------------------------------------------------

%\begin{frame}[fragile] % Need to use the fragile option when verbatim is used in the slide
%\frametitle{Verbatim}
%\begin{example}[Theorem Slide Code]
%\begin{verbatim}
%\begin{frame}
%\frametitle{Theorem}
%\begin{theorem}[Mass--energy equivalence]
%$E = mc^2$
%\end{theorem}
%\end{frame}\end{verbatim}
%\end{example}
%\end{frame}

%------------------------------------------------



%------------------------------------------------

%------------------------------------------------

\section{References}
\begin{frame}[allowframebreaks]
%\frametitle{References}
\footnotesize{
\bibliographystyle{plainurl}	
\bibliography{bibligraphy.bib}
}
\end{frame}

%------------------------------------------------

\begin{frame}
\Huge{\centerline{The End}}
\end{frame}

%----------------------------------------------------------------------------------------

\end{document} 
