%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% i7 Seminar Report Template
% Version June 23, 2023

\documentclass[a4paper,11pt,DIV=15]{scrartcl} % Do not edit this line.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Preamble

% Page Geometry, Typography and Encoding
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{microtype}
\renewcommand{\phi}{\varphi}
\renewcommand{\epsilon}{\varepsilon}
% \renewcommand{theta}{\vartheta} % if you want

% Math packages
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}

% Floats
\usepackage{float}
\usepackage{booktabs}
\usepackage{tikz}
\usetikzlibrary{positioning,arrows.meta}

% Colors
\usepackage{xcolor} %already loaded by tikz, but here for completeness
% RWTH colors
% blue violet purple carmine red magenta orange yellow grass cyan gold silver
\definecolor{rwth-blue}{cmyk}{1,.5,0,0}\colorlet{rwth-lblue}{rwth-blue!50}\colorlet{rwth-llblue}{rwth-blue!25}
\definecolor{rwth-violet}{cmyk}{.6,.6,0,0}\colorlet{rwth-lviolet}{rwth-violet!50}\colorlet{rwth-llviolet}{rwth-violet!25}
\definecolor{rwth-purple}{cmyk}{.7,1,.35,.15}\colorlet{rwth-lpurple}{rwth-purple!50}\colorlet{rwth-llpurple}{rwth-purple!25}
\definecolor{rwth-carmine}{cmyk}{.25,1,.7,.2}\colorlet{rwth-lcarmine}{rwth-carmine!50}\colorlet{rwth-llcarmine}{rwth-carmine!25}
\definecolor{rwth-red}{cmyk}{.15,1,1,0}\colorlet{rwth-lred}{rwth-red!50}\colorlet{rwth-llred}{rwth-red!25}
\definecolor{rwth-magenta}{cmyk}{0,1,.25,0}\colorlet{rwth-lmagenta}{rwth-magenta!50}\colorlet{rwth-llmagenta}{rwth-magenta!25}
\definecolor{rwth-orange}{cmyk}{0,.4,1,0}\colorlet{rwth-lorange}{rwth-orange!50}\colorlet{rwth-llorange}{rwth-orange!25}
\definecolor{rwth-yellow}{cmyk}{0,0,1,0}\colorlet{rwth-lyellow}{rwth-yellow!50}\colorlet{rwth-llyellow}{rwth-yellow!25}
\definecolor{rwth-grass}{cmyk}{.35,0,1,0}\colorlet{rwth-lgrass}{rwth-grass!50}\colorlet{rwth-llgrass}{rwth-grass!25}
\definecolor{rwth-green}{cmyk}{.7,0,1,0}\colorlet{rwth-lgreen}{rwth-green!50}\colorlet{rwth-llgreen}{rwth-green!25}
\definecolor{rwth-cyan}{cmyk}{1,0,.4,0}\colorlet{rwth-lcyan}{rwth-cyan!50}\colorlet{rwth-llcyan}{rwth-cyan!25}
\definecolor{rwth-teal}{cmyk}{1,.3,.5,.3}\colorlet{rwth-lteal}{rwth-teal!50}\colorlet{rwth-llteal}{rwth-teal!25}
\definecolor{rwth-gold}{cmyk}{.35,.46,.7,.35}
\definecolor{rwth-silver}{cmyk}{.39,.31,.32,.14}

% Hyperlinks and Cross-References
\usepackage{hyperref}
\usepackage[capitalise,noabbrev]{cleveref}
\hypersetup{%
	pdftoolbar=false,
	pdfmenubar=false,
	colorlinks,
	%pdfborderstyle={/S/U/W 1.25},
	urlcolor={rwth-magenta},
	linkcolor={rwth-red},
	citecolor={rwth-green}
}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{claim}[theorem]{Claim}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}



% Misc packages
\usepackage{lipsum}
\usepackage{mathrsfs}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Document


\begin{document}

%%TODO Insert topic of seminar, e.g. Theoretical Topics in Data Science or Complexity Theory
\subtitle{Seminar ``Theoretical Topics in Data Science''}
\date{\today}
\publishers{RWTH Aachen University}	% Do not edit this line.

%%TODO Change this to your report title.
\title{Latent Semantic Indexing: A Probabilistic Analysis}

%%TODO Change this to your name.
\author{Vahe Eminyan}

\maketitle


%%TODO Provide a short abstract for your report.
\begin{abstract}
This seminar paper gives an analysis of the scientific paper "Latent Semantic Indexing: A Probabilistic Analysis" by Papadimitriou et al.\\
Latent semantic indexing (LSI) is one of the techniques used for information retrieval.
LSI uses a mathematical technique called Singular Value Decomposition (SVD) applied to the term-document matrix. It is used to discover the hidden (latent) structure in the text data.\\
LSI is widely used in practice and has shown strong empirical results. However, due to the large size of term-document matrices, the computation can be slow.
These lead to two interesting questions: why does LSI perform so well, and how can we speed up computations?
Papadimitriou et al. address both of these questions.
They provide a theoretical justification for LSI's effectiveness, considering a special type of term-document matrix.\\
Additionally, to expedite computations, they suggest initially mapping the large matrices into low-dimensional spaces using random projections and then applying LSI to this reduced-dimensional projection.
They prove that the matrix obtained via random projection followed by LSI recovers almost as
much as the matrix obtained by direct LSI, with high probability.\\
In this seminar paper, we will consider all relevant information, providing a detailed analysis of LSI by random projection.





\end{abstract}

\thispagestyle{empty}

\clearpage

%%TODO The content of your report goes below.

\section{Introduction} %KES Ej
%In this section, we will introduce the topic and explain how the paper is structured.
%My plan is to cover all sections focusing on section 5.
Retrieving information from given data has been an important aspect for a long time.
Due to the development of information systems and the Internet, huge amounts of data are available. 
Handling these huge amounts of data is a great challenge, so many techniques have been developed to cope with the work process. 
Consider a scenario where we have a dataset comprising millions of documents structured as a term-document matrix and a user submits a query. Instead of just finding the documents that include the words of the query, the goal is to find documents that align with the query semantically. This semantic understanding ensures more accurate and relevant search results, even in extensive datasets.
Latent Semantic Indexing (LSI) is one of the information retrieval techniques. It uses Singular Value Decomposition (SVD) to represent the term-document matrix as a product of three matrices. By representing the term-document matrix in such a way we want to find the underlying (latent, hidden) semantical topics (also called concepts) of the term-document matrix. With the help of such decomposition, we can map the documents and queries to a lower dimensional space and compare them not only syntactically but also semantically. (Section 2 provides more detailed information about LSI and SVD).\\
LSI has shown strong empirical results. However, prior to the paper of Papadimitriou et al., there was no satisfactory explanation for its success (Why does LSI find the documents corresponding to the semantics of the query with high accuracy). They prove a theorem that under certain constraints LSI will always find the correct topic of the given query with high probability. Section 4 of this seminar paper elaborates on this theorem.\\
Despite its effectiveness, the computational time of LSI is very long. Papadimitriou et al. introduce a two-step LSI, in which we first map the original term-document matrix into a lower dimensional space by using random projections and then apply LSI. The paper proves that the matrix obtained via random projection followed by LSI recovers almost as much as the matrix obtained by direct LSI, with high probability. Section 5 provides a detailed analysis of LSI by random projection and presents the proof for this statement.\\
In the last section, we will draw a conclusion and give a short summary of this seminar paper.




 

\section{LSI Background} %MI EJ
%In this part we will explain the technique LSI, how it works, and its mathematical background (SVD).
The corpus is defined as a set of documents. Each document is a vector of length $n$ from $\mathbb{R}^n$. Each position of the document describes a mathematical function in terms of $i$th term of the entire term space. The function can, for example, be the frequency of the $i$th term, or just 0-1 ($1$ if the term appears in the document, $0$ otherwise).\\
Let $A \in \mathbb{R}^{n \times m}$ be a matrix of rank $r$. The Singular Value Decomposition (SVD) is a mathematical technique that represents the matrix as a product of three matrices.
\[
A = UDV^T.
\]
Let $\sigma_1 \geq \sigma_2 \geq ...\geq \sigma_r$ be the eigenvalues of the matrix $AA^T$ (called singular values of $A$), then $D = diag(\sigma_1,...,\sigma_r) \in \mathbb{R}^{r \times r}$. $U \in \mathbb{R}^{n \times r}$ is the matrix representing the eigenvectors of the matrix $AA^T$ and $V \in \mathbb{R}^{m \times r}$ the eigenvectors of the matrix $A^TA$. The columns of $U$ and $V$ are orthonormal. It is shown that every matrix can be represented in such a form [ZITATTTTT]\\
In LSI the matrix $A \in \mathbb{R}^{n \times m}$ represents a term-document matrix of $n$ terms and $m$ documents. Each row of the matrix corresponds to a term and each column to a document.\\
LSI keeps only $k$-largest singular values of the matrix considering only the first $k$ columns of matrices $U$ and $V$. I.e.
\[
A_k = U_kD_kV_k^T
\]
which is an approximation of the original matrix $A$. 
In order to find the most similar documents for the submitted query we map the query to the $k$-dimensional space by using the matrix $U_k$. Afterward, we compare the $k$-dimensional projection with $k$-dimensional representations of the documents in the corpus. For this comparison, we can use cosine similarity. Thus we only need to store the three matrices $U_k$, $D_k$, and $V_k$.
At this point, we can see the saving of the storage. In order to save the matrix $A$ entirely we need to memorize $n \cdot m$ entries. In contrast to that, we need only $n \cdot k + k\cdot k + m \cdot k$ entries, which is much smaller than  $n \cdot m$, because $k$ is much smaller than both $n$ and $m$.\\
The approximation $A_k$ of rank $k$ is the matrix that minimizes the Frobenius norm of the  of $A - A_k$. Formally:
\begin{theorem}
	[ZITATTT]Among all $n \times m$ matrices C of rank at most $k$, $A_k$ is the one that minimizes $||A - C||_F^2 = \sum_{i,j}(A_{ij} - C_{ij})^2$.
\end{theorem}
I.e. this approximation preserves all relative distances, now we only need to say why it finds queries and documents that are semantically related.








 

\section{Probabilistic Corpus Model} %Kes Ej: Since corpus is used for the explanation of both aspects it is more a theoretical framework.
Because LSI relies on uncovering the statistical characteristics of a corpus, it is essential to begin with a well-defined probabilistic model of the corpus.
In this section, we introduce the probabilistic corpus model.
%We want to analyze a corpus (a term-document matrix) by capturing its probabilistic properties. Therefore, a formal corpus model is introduced in this section. 
In the rest of the paper, we assume that the corpus is generated from a corpus model.

\begin{definition}
The universe $U$ is the set of all terms.
\end{definition}
\begin{definition}
A topic is a probability distribution on $U$.
\end{definition}
The logical interpretation of this definition is the following. Assume we have a set of documents where one partition is about computer science and the other is about nature. Hence there would be two hidden topics for the whole set of documents.
Hence the one topic will probably include terms like computer, software and algorithm, and the second will include the terms forest, ocean and earth. Thus we can interpret a topic as a probability distribution on the entire set of the terms.

Another important aspect is the authorship style, it influences the frequencies of words and hence the documents. 
\begin{definition}
    A style is a $|U| \times|U|$ stochastic matrix (a matrix with nonnegative entries and row sums equal to 1), denoting the way whereby style modifies
the frequency of terms.
\end{definition}

After introducing the previous three definitions, now we can define the probabilistic corpus model.
\begin{definition}
     A corpus model $\mathscr{C}$ is a quadruple $\mathscr{C} = (U,\mathscr{T}, \mathscr{S}, D)$, where $U$ is
the universe of terms, $\mathscr{T}$ is a set of topics, and $\mathscr{S}$ is a set of styles, and $D$ a probability distribution on $\hat{\mathscr{T}} \times \hat{\mathscr{S}} \times \mathbb{Z}^+$, where by $\hat{\mathscr{T}}$ we denote the set of all convex combinations of topics in $\mathscr{T}$, by $\hat{\mathscr{S}}$ the set of all convex combinations of styles in $\mathscr{S}$, and by 
$\mathbb{Z}^+$ the set of positive integers (the integers represent the lengths of documents).
\end{definition}
I.e. a corpus generated from the introduced corpus model is a set of documents in which every document is generated as follows: we sample a triple from $D$. It includes fixed $\hat{T} \in \mathscr{T}$, $\hat{S} \in \mathscr{S}$, and a fixed length $\ell$ of the document. Then we sample the terms of the document $\ell$ times according to $\hat{T}$.


\section{Brief Mention of Analysis of LSI's Good Performance} %KEs Eji see greek paper
%In this part, I will provide the most significant information of paragraph 4 of the original paper. \\
%Main idea: Under certain conditions and assumptions, it can be shown why LSI performs well.
In this section, we provide theorem that states under certain conditions the LSI brings similar documents together.
First, we need a set of definitions for a corpus model.
\begin{definition}
  A corpus model $\mathscr{C}$ is pure if each document involves a single topic.
\end{definition}

\begin{definition}
  A corpus model $\mathscr{C}$ is $\epsilon$-seperable ($\epsilon \in [0,1)$), if a set of terms $U_T$ is associated with each topic $T \in \mathscr{T}$ in the following way:
\begin{itemize}
    \item $U_T$ are mutually disjoint,
    \item for each $T$, the total probability $T$ assign to the terms in $U_T$  is at least $1-\epsilon$
\end{itemize}
We call $U_T$ the primary set of terms of topic $T$.
\end{definition}

In this section, we assume that the corpus model is style-free.
Let $\mathscr{C} = (U,\mathscr{T}, D)$ be a pure and style-free corpus model and let $k = |\mathscr{T}|$ denote the number of topics in $\mathscr{C}$. Since $\mathscr{C}$ is pure, each document generated from $\mathscr{C}$ is in fact generated from some single topic $T$: hence we say that the document belongs to topic $T$.
Let C be a corpus generated from $\mathscr{C}$ and, for each document $d \in C$, let $v_d$ denote the vector assigned to $d$ by the rank-$k$ LSI performed on $C$.



\begin{definition}
    Rank-$k$ LSI is $\delta$-skewed on the corpus instance $C$ if, for each pair of documents $d$ and $d'$, $v_d \cdot v_{d'} \leq \delta \lVert v_d \rVert \lVert v_{d'}\rVert$,  if $d$ and $d'$ belong to different topics and $v_d \cdot v_{d'} \geq 1 - \delta  \lVert v_d \rVert \lVert v_{d'}\rVert$  if they belong to the same topic.
\end{definition}
The following theorem shows that under some constraints on the corpus model the rank-$k$ LSI indeed does well with high probability.

\begin{theorem}
    Let $\mathscr{C}$ be a pure, $\epsilon$-seperable corpus model with $k$ topics such that the probability each topic assigns to each term is at most $\tau$, where $\tau > 0$ is a sufficiently small constant. Let $C$ be a corpus of $m$ documents generated from $\mathscr{C}$. Then, the rank-$k$ LSI is $O(\epsilon)$-skewed on $C$ with probability $1-O(m^{-1})$.
\end{theorem}

In this seminar paper, we will omit the proof of this theorem. It can be found in the original paper.







\section{LSI by Random Projection} %Ereq EJ
In order to reduce the computational time we use dimensionality reduction and then apply LSI.
According to Johnson and Lindenstrauss's lemma [ZiTAAAT]  random projection of a matrix to a lower dimensional space preserves pairwise distances of its element while representing each element in lower dimensional space. However, it does not bring semantically connected elements (i.e. documents together).  Papadimitriou et al. suggest a two-step LSI:
\begin{enumerate}
    \item  Apply a random projection to the initial corpus to $\ell$ dimensions, for some
small $\ell > k$, to obtain, with high probability, a much smaller representation, which
is still very close (in terms of distances and angles) to the original corpus.

\item Apply rank $O(k)$ LSI (because of the random projection, the number of
singular values kept may have to be increased a little)
\end{enumerate}

Before diving into the main theorem of this chapter, let's cover some formal groundwork.
We again consider $A \in \mathbb{R}^{n \times m}$ which represents our term-document matrix and is generated from the corpus model. Let $R \in \mathbb{R}^{n \times l} $ be a random column-orthonormal matrix. We use $R$ to project $A$ into $\ell$-dimensional space. We define $B:= \sqrt{n/\ell}R^TA$ as the scaled random projection of $A$.
The SVD representations of A and B are denoted as follows:
\[
A = \sum_{i=1}^r \sigma_i u_i v_i \,\,\,\,\,\,\,\,\text{and}\,\,\,\,\,\,\,\, B=\sum_{i=1}^t \lambda_i a_i b_i.
\]
We also introduce the following lemma with its corollary.
\begin{lemma}
Let $\epsilon$ be an arbitrary positive constant. If $\ell \geq c((\log n)/\epsilon^2)$ for a sufficiently large constant $c$ then, for  $p =1,...t$
\[
\lambda_p^2 \geq \dfrac{1}{k} [(1-\epsilon) \sum_{i=1}^k \sigma_i^2 - \sum_{j=1}^{p-1} \lambda_j^2].
\]
\end{lemma}
\begin{corollary}
\[
\sum_{p=1}^{2k} \lambda_p^2 \geq (1-\epsilon) \lVert A_k \rVert_F^2.
\]
\end{corollary}
\begin{theorem}
    Parsevals identity
\end{theorem}
\begin{theorem}
    Eckart-Young-Minsky theorem.
\end{theorem}

Our rank-$2k$ approximation of matrix $A$ is denoted as
\[
B_{2k} := A \sum_{i=1}^{2k} b_ib_i^T
\]

Now we can introduce the main theorem of this section.
\begin{theorem}
\[
    \lVert A - B_{2k} \rVert_F^2 \leq \lVert A - A_k \rVert_F^2 + 2\epsilon\lVert A \rVert_F^2
\]
Informally, the theorem states that the original matrix $A$ after applying random projection and then LSI is almost as good recovered as by using LSI on the original matrix.
\end{theorem}

\begin{proof}
We have 
\[
A = \sum_{i=1}^r \sigma_i u_i v_i \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,A_k = \sum_{i=1}^k \sigma_i u_i v_i
\]

\[
 B=\sum_{i=1}^t \lambda_i a_i b_i\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, B_{2k} := A \sum_{i=1}^{2k} b_ib_i^T.
\]

$b_1,...,b_n$ is an orthonormal set of vectors spanning the row space of $A$, hence using the Parseval's identity we can write
\[
 \lVert A - B_{2k} \rVert_F^2 = \sum_{i=1}^n |(A - B_{2k})b_i|^2
\]
for $i = 1,...,2k$, because $b_i^Tb_i = 1$ we have
\[
(A - B_{2k})b_i = Ab_i - Ab_i = 0,
\]
and for  $i = 2k+1,...,n$, because $b_j^Tb_i = 0$ we have
\[
(A - B_{2k})b_i = Ab_i.
\]
Hence

\begin{align*}
    \lVert A - B_{2k} \rVert_F^2 &= \sum_{i=1}^n |(A - B_{2k})b_i|^2\\
    &= \sum_{i=2k+1}^n |A b_i|^2 = \sum_{i=1}^n |A b_i|^2 - \sum_{i=1}^{2k} |A b_i|^2\\
    &\overset{\text{Parseval's id.}}{=} \lVert A \rVert_F^2 - \sum_{i=1}^{2k} |A b_i|^2 
\end{align*}

On the other hand, we have
\begin{align*}
 \lVert A - B_{2k} \rVert_F^2 &\overset{\text{E-Y-M theorem}}{=} \sum_{i=k+1}^n \sigma_i^2 \\
 &\overset{\text{Frob. norm def.}}{=}  \lVert A \rVert_F^2 - \lVert A_k \rVert_F^2. 
\end{align*}

Now we consider 
\begin{align*}
   \lVert A - B_{2k} \rVert_F^2 - \lVert A - A_k \rVert_F^2 &= \lVert A \rVert_F^2 - \sum_{i=1}^{2k} |A b_i|^2  - (\lVert A \rVert_F^2 - \lVert A_k \rVert_F^2)\\ 
   &= \lVert A_k \rVert_F^2 - \sum_{i=1}^{2k} |A b_i|^2
\end{align*}
That is equivalent to
\[
\lVert A - B_{2k} \rVert_F^2 = \lVert A - A_k \rVert_F^2 + (\lVert A_k \rVert_F^2 - \sum_{i=1}^{2k} |A b_i|^2)
\]

For the next step, we show
\[
(1+\epsilon) \sum_{i=1}^{2k} |A b_i|^2 \geq \sum_{i=1}^{2k} \lambda_i^2.
\]
\begin{align*}
    \sum_{i=1}^{2k} \lambda_i^2 \overset{|Bb_i| = \lambda_i}{=}\sum_{i=1}^{2k}|Bb_i|^2 
    \overset{\text{sbst. B}}{=} \sum_{i=1}^{2k} | \sqrt{\dfrac{n}{\ell}} R^T (Ab_i)|^2
    = \sum_{i=1}^{2k} \dfrac{n}{\ell}| R^T (Ab_i)|^2
\end{align*}

Now from the Random projection theorem for very large $\ell$ we have for each $i$
\[
 \dfrac{n}{\ell}| R^T (Ab_i)|^2 \leq (1+\epsilon) |Ab_i|^2
\]
with high probability.

Hence with a high probability
\[
(1+\epsilon) \sum_{i=1}^{2k} |A b_i|^2 \geq \sum_{i=1}^{2k} \lambda_i^2.
\]

Now we have
\begin{align*}
    \sum_{i=1}^{2k} |A b_i|^2 \geq \dfrac{1}{(1+\epsilon)} \sum_{i=1}^{2k} \lambda_i^2 
    \overset{\text{Cor.11}}{\geq} \dfrac{(1-\epsilon)}{(1+\epsilon)} \lVert A_k \rVert_F^2 
    \geq (1-2\epsilon) \lVert A_k \rVert_F^2
\end{align*}
I.e.
\[
\sum_{i=1}^{2k} |A b_i|^2 \geq (1-2\epsilon) \lVert A_k \rVert_F^2
\]

Now we substitute this result in [BUILD REFFFFFF] getting
\begin{align*}
    \lVert A - B_{2k} \rVert_F^2 \leq \lVert A - A_k \rVert_F^2 + \lVert A_k \rVert_F^2 - (1-2\epsilon) \lVert A_k \rVert_F^2\\
\iff \lVert A - B_{2k} \rVert_F^2 \leq \lVert A - A_k \rVert_F^2 + 2\epsilon\lVert A_k \rVert_F^2
\end{align*}

Due to the definition of Frobenius norm, we have  $\lVert A \rVert_F^2 \geq \lVert A_k \rVert_F^2$. This inequality leads us to the conclusive result of our proof
\[
\lVert A - B_{2k} \rVert_F^2 \leq \lVert A - A_k \rVert_F^2 + 2\epsilon\lVert A \rVert_F^2.
\]


%%%%%%%%%%%%%%%%%%%%%%%%%%REFORMULATE%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
What are the computational savings achieved by the two-step method? Let A be an nm matrix. The time to compute LSI is O(mnc) if A is sparse with about c
nonzero entries per column (i.e., c is the average number of terms in a document).
The time needed to compute the random projection to l dimensions is O(mcl). After
the projection, the time to compute LSI is O(ml2
). So the total time is O(ml(l+c)).
To obtain an = approximation we need l to be O((log n)=2
). Thus the running time
of the two-step method is asymptotically superior: O(m(log2 n+c log n)) compared
to O(mnc).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





\end{proof}

%We prove theorem 5 of the paper which maintains that the matrix obtained via random projection followed by LSI recovers almost as
%much as the matrix obtained by direct LSI, with high probability.



\section{Conclusion and Summary} %Kes EJic 
In this section, we will draw conclusions based on the findings presented in the seminar paper and give a brief summary of the paper.\\
\\
\\
\\
\\
Let $A \in \mathbb{R}^{n \times m}$ be a matrix and $b_1,...,b_n$ an orthonormal set of vectors where $b_i \in \mathbb{R}^{m}$.
$F$ is the Frobenius norm of a matrix. Why is the following correct? $$||A||_F^2 = \sum_{i=1}^n |Ab_i|^2$$












\clearpage

\bibliographystyle{plainurl}
\bibliography{references.bib}





\end{document}





