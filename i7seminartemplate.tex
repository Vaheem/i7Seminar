%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% i7 Seminar Report Template
% Version June 23, 2023

\documentclass[a4paper,11pt,DIV=15]{scrartcl} % Do not edit this line.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Preamble

% Page Geometry, Typography and Encoding
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{microtype}
\renewcommand{\phi}{\varphi}
\renewcommand{\epsilon}{\varepsilon}
% \renewcommand{theta}{\vartheta} % if you want

% Math packages
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}

% Floats
\usepackage{float}
\usepackage{booktabs}
\usepackage{tikz}
\usetikzlibrary{positioning,arrows.meta,matrix,decorations.pathreplacing,calc,positioning}


% Colors
\usepackage{xcolor} %already loaded by tikz, but here for completeness
% RWTH colors
% blue violet purple carmine red magenta orange yellow grass cyan gold silver
\definecolor{rwth-blue}{cmyk}{1,.5,0,0}\colorlet{rwth-lblue}{rwth-blue!50}\colorlet{rwth-llblue}{rwth-blue!25}
\definecolor{rwth-violet}{cmyk}{.6,.6,0,0}\colorlet{rwth-lviolet}{rwth-violet!50}\colorlet{rwth-llviolet}{rwth-violet!25}
\definecolor{rwth-purple}{cmyk}{.7,1,.35,.15}\colorlet{rwth-lpurple}{rwth-purple!50}\colorlet{rwth-llpurple}{rwth-purple!25}
\definecolor{rwth-carmine}{cmyk}{.25,1,.7,.2}\colorlet{rwth-lcarmine}{rwth-carmine!50}\colorlet{rwth-llcarmine}{rwth-carmine!25}
\definecolor{rwth-red}{cmyk}{.15,1,1,0}\colorlet{rwth-lred}{rwth-red!50}\colorlet{rwth-llred}{rwth-red!25}
\definecolor{rwth-magenta}{cmyk}{0,1,.25,0}\colorlet{rwth-lmagenta}{rwth-magenta!50}\colorlet{rwth-llmagenta}{rwth-magenta!25}
\definecolor{rwth-orange}{cmyk}{0,.4,1,0}\colorlet{rwth-lorange}{rwth-orange!50}\colorlet{rwth-llorange}{rwth-orange!25}
\definecolor{rwth-yellow}{cmyk}{0,0,1,0}\colorlet{rwth-lyellow}{rwth-yellow!50}\colorlet{rwth-llyellow}{rwth-yellow!25}
\definecolor{rwth-grass}{cmyk}{.35,0,1,0}\colorlet{rwth-lgrass}{rwth-grass!50}\colorlet{rwth-llgrass}{rwth-grass!25}
\definecolor{rwth-green}{cmyk}{.7,0,1,0}\colorlet{rwth-lgreen}{rwth-green!50}\colorlet{rwth-llgreen}{rwth-green!25}
\definecolor{rwth-cyan}{cmyk}{1,0,.4,0}\colorlet{rwth-lcyan}{rwth-cyan!50}\colorlet{rwth-llcyan}{rwth-cyan!25}
\definecolor{rwth-teal}{cmyk}{1,.3,.5,.3}\colorlet{rwth-lteal}{rwth-teal!50}\colorlet{rwth-llteal}{rwth-teal!25}
\definecolor{rwth-gold}{cmyk}{.35,.46,.7,.35}
\definecolor{rwth-silver}{cmyk}{.39,.31,.32,.14}

% Hyperlinks and Cross-References
\usepackage{hyperref}
\usepackage[capitalise,noabbrev]{cleveref}
\hypersetup{%
	pdftoolbar=false,
	pdfmenubar=false,
	colorlinks,
	%pdfborderstyle={/S/U/W 1.25},
	urlcolor={rwth-magenta},
	linkcolor={rwth-red},
	citecolor={rwth-green}
}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{claim}[theorem]{Claim}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}



% Misc packages
\usepackage{lipsum}
\usepackage{mathrsfs}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Document


\begin{document}

%%TODO Insert topic of seminar, e.g. Theoretical Topics in Data Science or Complexity Theory
\subtitle{Seminar ``Theoretical Topics in Data Science''}
\date{\today}
\publishers{RWTH Aachen University}	% Do not edit this line.

%%TODO Change this to your report title.
\title{Latent Semantic Indexing: A Probabilistic Analysis}

%%TODO Change this to your name.
\author{Vahe Eminyan}

\maketitle


%%TODO Provide a short abstract for your report.
\begin{abstract}
This seminar paper gives an analysis of the work of  Papadimitriou et al. "Latent Semantic Indexing: A Probabilistic Analysis" \cite{APAPADIMITRIOU2000217}.

Latent semantic indexing (LSI) is one of the techniques used for information retrieval.
LSI uses a mathematical technique called Singular Value Decomposition (SVD) applied to the term-document matrix. It is used to discover the hidden (latent) structure in the text data.

LSI is widely used in practice and has shown strong empirical results \cite{LSIusage1, LSIusage2, LSIusage3}. However, due to the large size of term-document matrices, the computation can be slow. This raises two interesting questions: why does LSI perform so well, and how can we speed up computations?
Papadimitriou et al. address both of these questions.
They provide a theoretical justification for LSI's effectiveness, considering a special type of term-document matrix.

Additionally, to expedite computations, they suggest initially mapping the large matrices into low-dimensional spaces using random projections and then applying LSI to this reduced-dimensional projection.
They prove that the matrix obtained via random projection followed by LSI recovers almost as
much as the matrix obtained by direct LSI, with high probability.

In this seminar paper, we delve into both aforementioned aspects, with a specific emphasis on the second question. 





\end{abstract}

\thispagestyle{empty}

\clearpage

%%TODO The content of your report goes below.

\section{Introduction} %KES Ej
%In this section, we will introduce the topic and explain how the paper is structured.
%My plan is to cover all sections focusing on section 5.
Retrieving information from given data is an important aspect.
Due to the development of information systems and the digital landscape, huge amounts of data are available. 
Handling these huge amounts of data is a great challenge, so many techniques have been developed to cope with the work process. 
Consider a scenario where we have a dataset comprising millions of documents structured as a term-document matrix and a user submits a query. Instead of just finding the documents that include the words of the query, the goal is to find documents that align with the query semantically. Another example is to build a recommendation system form a movie-user matrix.  %Another example is given a user-movie matrix, we aim to create a recommendation system.
This semantic understanding ensures more accurate and relevant search results, even in extensive datasets.
Latent Semantic Indexing (LSI) is one of the information retrieval techniques. It uses Singular Value Decomposition (SVD) to represent the term-document matrix as a product of three matrices. By representing the term-document matrix in such a way we want to find the underlying (latent, hidden) semantical topics (also called concepts) of the term-document matrix. With the help of such decomposition, we can map the documents and queries to a lower dimensional space and compare them not only syntactically but also semantically. (\cref{LSI Background} provides more detailed information about LSI and SVD).

LSI has shown strong empirical results. However, prior to the paper of Papadimitriou et al., there was no satisfactory explanation for its success. The question is: Why does LSI find the documents corresponding to the semantics of the query with high accuracy. They prove a theorem that under certain constraints LSI will always find the correct topic of the given query with high probability. \cref{Brief Mention of Analysis of LSI's Good Performance} of this seminar paper elaborates on this theorem.

Despite its effectiveness, the computational time of LSI is very long. Papadimitriou et al. introduce a two-step LSI, in which we first map the original term-document matrix into a lower dimensional space by using random projections and then apply LSI. The paper proves that the matrix obtained via random projection followed by LSI recovers almost as much as the matrix obtained by direct LSI, with high probability.  \cref{LSI by Random Projection} provides a detailed analysis of LSI by random projection and presents the proof for this statement.
In the last section, we will draw a conclusion and give a short summary.

\section{Related Work}
Hofmann et al. analyze the LSI from a different probabilistic point of view comparing SVD to mixture decomposition using expectation maximization algorithm \cite{Relatedwork1}.
Despite analyzing the LSI's good performance, Frieze et al. give another method to speed up LSI, introducing an approach that includes Fast-Monte-Carlo algorithms \cite{Relatedwork2}. More LSI insights can be found in \cite{SVDNEW1, SVDNEW2, SVDNEW3, RELATED}. Newer information retrieval methods are given in \cite{GNN, HYLe17}.





 

\section{LSI Background} \label{LSI Background} %MI EJ
%In this part we will explain the technique LSI, how it works, and its mathematical background (SVD).
The corpus is defined as a set of documents. Each document is a vector of length $n$ from $\mathbb{R}^n$. Each position of the document describes a mathematical function in terms of $i$th term of the entire term space. The function can, for example, be the frequency of the $i$th term, or just 0-1 ($1$ if the term appears in the document, $0$ otherwise).

Let $A \in \mathbb{R}^{n \times m}$ be a matrix of rank $r$. The Singular Value Decomposition (SVD) is a mathematical technique that represents the matrix as a product of three matrices
\[
A = UDV^T = \sum_{i=1}^r \sigma_i u_i v_i^T  
\]
Where $\sigma_1 \geq \sigma_2 \geq ...\geq \sigma_r$ are the eigenvalues of the matrix $AA^T$ (called singular values of $A$), $D = diag(\sigma_1,...,\sigma_r) \in \mathbb{R}^{r \times r}$. $U \in \mathbb{R}^{n \times r}$ is the matrix representing the eigenvectors of the matrix $AA^T$ and $V \in \mathbb{R}^{m \times r}$ the eigenvectors of the matrix $A^TA$. The columns of $U$ and $V$ are orthonormal \ref{eq:matrixA}. Every matrix can be represented in this form \cite{strang2005}.
\begin{equation}
    A = 
\Large
\begin{pmatrix}
    \phantom{-} \vert \phantom{-} & \phantom{-} \vert \phantom{-} & & \phantom{-} \vert \phantom{-} \\
    u_1 & u_2 & \cdots & u_r \\
    \phantom{-} \vert \phantom{-} & \phantom{-} \vert \phantom{-} & & \phantom{-} \vert \phantom{-} \\
\end{pmatrix}
\normalsize
\begin{pmatrix}
    \sigma_1 & 0 & \cdots & 0 \\
    0 & \sigma_2 & \cdots & 0 \\
    \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & \cdots & \sigma_r
\end{pmatrix}
%\small 
\begin{pmatrix}
    \phantom{-} \big\lvert \phantom{-} & \phantom{-} \vert \phantom{-} & & \phantom{-} \vert \phantom{-} \\
    v_1 & v_2 & \cdots & v_r \\
    \phantom{-} \vert \phantom{-} & \phantom{-} \vert \phantom{-} & & \phantom{-} \vert \phantom{-} \\
\end{pmatrix}^T
\label{eq:matrixA}
\end{equation}









Now the matrix $A \in \mathbb{R}^{n \times m}$ represents a term-document matrix of $n$ terms and $m$ documents. Each row of the matrix corresponds to a term and each column to a document. $U$ can be interpreted as "term-topic similarity" matrix, where each row represents how the given word is related to each of the $r$-topics. $V$ can be interpreted as "document-topic similarity" matrix, where each row represents how the given document relates to each $r$-topics. Each diagonal element of the matrix  $D$ shows the relevance of each topic ordered in decreasing order (from more relevant to less relevant).

Rank-$k$ LSI keeps only $k$-largest singular values of the matrix considering only the first $k$ columns of matrices $U$ and $V$($k$ defines the top $k$ most important topics/concepts of the term-document matrix). I.e.
\[
A_k = U_kD_kV_k^T
\]
which is an approximation of the original matrix $A$. 
In order to find the most similar documents for the submitted query we map the query to the $k$-dimensional space by using the matrix $U_k$. Afterward, we compare the $k$-dimensional projection with $k$-dimensional representations of the documents in the corpus. For this comparison, we can use cosine similarity. Thus we only need to store the three matrices $U_k$, $D_k$, and $V_k$.
At this point, we can see the saving of the storage. In order to save the matrix $A$ entirely we need to memorize $n \cdot m$ entries. In contrast to that, we need only $n \cdot k + k\cdot k + m \cdot k$ entries, which is much smaller than  $n \cdot m$, because $k$ is much smaller than both $n$ and $m$.

The rank $k$ approximation of matrix $A$ denoted as $A_k$  is the matrix that minimizes the Frobenius norm of $A - A_k$. Formally:
\begin{theorem}
	\cite{EckartYoung} Among all $n \times m$ matrices C of rank at most $k$, $A_k$ is the one that minimizes $||A - C||_F^2 = \sum_{i,j}(A_{ij} - C_{ij})^2$.
\end{theorem}
I.e. this approximation preserves all relative distances, now we only need to show why it finds queries and documents that are semantically related.








 

\section{Probabilistic Corpus Model} %Kes Ej: Since corpus is used for the explanation of both aspects it is more a theoretical framework.
Because LSI relies on uncovering the statistical characteristics of a corpus, it is essential to begin with a well-defined probabilistic model of the corpus.
In this section, we introduce the probabilistic corpus model.
%We want to analyze a corpus (a term-document matrix) by capturing its probabilistic properties. Therefore, a formal corpus model is introduced in this section. 
In the rest of the paper, we assume that the corpus is generated from a corpus model.
\begin{definition}
A topic is a probability distribution on the universe of all terms $U$.
\end{definition}
The logical interpretation of this definition is the following. Assume we have a set of documents where one partition is about computer science and the other is about nature. Hence there would be two hidden topics for the whole set of documents.
Hence the one topic will probably include terms like computer, software and algorithm, and the second will include the terms forest, ocean and earth. Thus we can interpret a topic as a probability distribution on the entire set of the terms.

Another important aspect is the authorship style, it influences the frequencies of words and hence the documents. 
\begin{definition}
    A style is a $|U| \times|U|$ stochastic matrix (a matrix with nonnegative entries and row sums equal to 1), denoting the way whereby style modifies
the frequency of terms.
\end{definition}

After introducing the previous three definitions, now we can define the probabilistic corpus model.
\begin{definition}
     A corpus model $\mathscr{C}$ is a quadruple $\mathscr{C} = (U,\mathscr{T}, \mathscr{S}, D)$, where $U$ is
the universe of terms, $\mathscr{T}$ is a set of topics, and $\mathscr{S}$ is a set of styles, and $D$ a probability distribution on $\hat{\mathscr{T}} \times \hat{\mathscr{S}} \times \mathbb{Z}^+$, where by $\hat{\mathscr{T}}$ we denote the set of all convex combinations of topics in $\mathscr{T}$, by $\hat{\mathscr{S}}$ the set of all convex combinations of styles in $\mathscr{S}$, and by 
$\mathbb{Z}^+$ the set of positive integers (the integers represent the lengths of documents).
\end{definition}
I.e. a corpus generated from the introduced corpus model is a set of documents in which every document is generated as follows: we sample a triple from $D$. It includes fixed $\hat{T} \in \mathscr{T}$, $\hat{S} \in \mathscr{S}$, and a fixed length $\ell$ of the document. Then we sample the terms of the document $\ell$ times according to $\hat{T}$ \cite{APAPADIMITRIOU2000217}.


\section{Brief Mention of Analysis of LSI's Good Performance} \label{Brief Mention of Analysis of LSI's Good Performance} %KEs Eji see greek paper
%In this part, I will provide the most significant information of paragraph 4 of the original paper. \\
%Main idea: Under certain conditions and assumptions, it can be shown why LSI performs well.
In this section, we introduce the theorem that states under certain conditions the LSI brings similar documents together.
First, we need a set of definitions for a corpus model.
\begin{definition}
  A corpus model $\mathscr{C}$ is pure if each document involves a single topic.
\end{definition}

\begin{definition}
  A corpus model $\mathscr{C}$ is $\epsilon$-separable ($\epsilon \in [0,1)$), if a set of terms $U_T$ is associated with each topic $T \in \mathscr{T}$ in the following way:
\begin{itemize}
    \item $U_T$ are mutually disjoint,
    \item for each $T$, the total probability $T$ assign to the terms in $U_T$  is at least $1-\epsilon$
\end{itemize}
We call $U_T$ the primary set of terms of topic $T$.
\end{definition}

In this section, we assume that the corpus model is style-free.
Let $\mathscr{C} = (U,\mathscr{T}, D)$ be a pure and style-free corpus model and let $k = |\mathscr{T}|$ denote the number of topics in $\mathscr{C}$. Since $\mathscr{C}$ is pure, each document generated from $\mathscr{C}$ is in fact generated from some single topic $T$: hence we say that the document belongs to topic $T$.
Let C be a corpus generated from $\mathscr{C}$ and, for each document $d \in C$, let $v_d$ denote the vector assigned to $d$ by the rank-$k$ LSI performed on $C$.



\begin{definition}
    Rank-$k$ LSI is $\delta$-skewed on the corpus instance $C$ if, for each pair of documents $d$ and $d'$, $v_d \cdot v_{d'} \leq \delta \lVert v_d \rVert \lVert v_{d'}\rVert$,  if $d$ and $d'$ belong to different topics and $v_d \cdot v_{d'} \geq 1 - \delta  \lVert v_d \rVert \lVert v_{d'}\rVert$  if they belong to the same topic.
\end{definition}
The following theorem shows that under some constraints on the corpus model the rank-$k$ LSI indeed does well with high probability.

\begin{theorem}
\label{theorem: lsiperform}
    Let $\mathscr{C}$ be a pure, $\epsilon$-seperable corpus model with $k$ topics such that the probability each topic assigns to each term is at most $\tau$, where $\tau > 0$ is a sufficiently small constant. Let $C$ be a corpus of $m$ documents generated from $\mathscr{C}$. Then, the rank-$k$ LSI is $O(\epsilon)$-skewed on $C$ with probability $1-O(m^{-1})$.
\end{theorem}

The proof of this theorem can be found in the original paper \cite{APAPADIMITRIOU2000217}.







\section{LSI by Random Projection} \label{LSI by Random Projection} %Ereq EJ
In order to reduce the computational time we use dimensionality reduction and then apply LSI.
According to Johnson and Lindenstrauss's lemma \cite{johnson1984Lindenstrauss}  random projection of a matrix to a lower dimensional space preserves pairwise distances of its element while representing each element in lower dimensional space. However, it does not bring semantically connected elements (i.e. documents together).  Papadimitriou et al. suggest a two-step LSI:
\begin{enumerate}
    \item 
Apply a random projection onto $\ell$ dimensions, where $\ell$ is a small value greater than $k$, on the initial corpus. This process, with high probability, yields a significantly smaller representation that maintains close proximity to the original corpus in both distances and angles.


\begin{equation}
    B = 
\sqrt{\dfrac{n}{\ell}}
\cdot
\begin{pmatrix}
    \phantom{-} \big\lvert \phantom{-} & \phantom{-} \vert \phantom{-} & & \phantom{-} \vert \phantom{-} \\
    r_1 & r_2 & \cdots & r_{\ell} \\
    \phantom{-} \vert \phantom{-} & \phantom{-} \vert \phantom{-} & & \phantom{-} \vert \phantom{-} \\
\end{pmatrix}^T
\cdot
A
%\small 
%\begin{pmatrix}
%    \phantom{-} \big\lvert \phantom{-} & \phantom{-} \vert \phantom{-} & & \phantom{-} \vert \phantom{-} \\
%    a_1 & a_2 & \cdots & a_m \\
%    \phantom{-} \vert \phantom{-} & \phantom{-} \vert \phantom{-} & & \phantom{-} \vert \phantom{-} \\
%\end{pmatrix}
\label{eq:matrixB}
\end{equation}

\item Apply rank $O(k)$ LSI (because of the random projection, the number of
singular values kept may have to be increased a little).
\end{enumerate}

Before diving into the main theorem of this chapter, let's cover some formal groundwork.
We again consider $A \in \mathbb{R}^{n \times m}$ which represents our term-document matrix and is generated from the corpus model. Let $R \in \mathbb{R}^{n \times l} $ be a random column-orthonormal matrix. We use $R$ to project $A$ into $\ell$-dimensional space. We define $B:= \sqrt{n/\ell}R^TA$ as the scaled random projection of $A$, see Equation \eqref{eq:matrixB}.
The SVD representations of A and B are denoted as follows:
\[
A = \sum_{i=1}^r \sigma_i u_i v_i^T \,\,\,\,\,\,\,\,\text{and}\,\,\,\,\,\,\,\, B=\sum_{i=1}^t \lambda_i a_i b_i^T.
\]
We also introduce two lemmas, a corollary and a theorem, which we use in our proof of the main theorem \ref{theorem:imp}.
\begin{lemma}
Let $\epsilon$ be an arbitrary positive constant. If $\ell \geq c((\log n)/\epsilon^2)$ for a sufficiently large constant $c$ then, for  $p =1,...t$
\[
\lambda_p^2 \geq \dfrac{1}{k} \left[(1-\epsilon) \sum_{i=1}^k \sigma_i^2 - \sum_{j=1}^{p-1} \lambda_j^2\right].
\]
\end{lemma}
\begin{corollary}
\label{cor:10}
\[
\sum_{p=1}^{2k} \lambda_p^2 \geq (1-\epsilon) \lVert A_k \rVert_F^2.
\]
\end{corollary}

\begin{lemma}
\label{lem:A_Ak}
\[
\lVert A - A_k \rVert_F^2  = \sum_{i=k+1}^n \sigma_i^2. 
\]
\end{lemma} 
\begin{proof}
\[
\lVert A - A_k \rVert_F^2 = \lVert \sum_{i=k+1}^n \sigma_i u_i v_i^T \rVert_F^2  = \sum_{i=k+1}^n \sigma_i^2 
\]
The second equality holds since the squared Frobenius norm of every matrix can be written as sum of squares of its singular values \cite{FrobeniusDef}.
\end{proof}

\begin{theorem}
    Parsevals identity \cite{hogben2013handbook}:
    Let ${b_1,...,b_n}$ be an orthonormal basis for a space $S$. Then for each $s \in S$, $|s|^2 = \sum_{i=1}^n (sb_i)^2$.
\end{theorem}



Our rank-$2k$ approximation of matrix $A$ is denoted as
\[
B_{2k} := A \sum_{i=1}^{2k} b_ib_i^T
\]

Now we can introduce the main theorem of this section:
\begin{theorem}
\label{theorem:imp}
\[
    \lVert A - B_{2k} \rVert_F^2 \leq \lVert A - A_k \rVert_F^2 + 2\epsilon\lVert A \rVert_F^2
\]
where $\epsilon \in (0,0.5)$
\end{theorem}
Informally, the theorem states that the original matrix $A$ after applying random projection and then LSI is almost as good recovered as by using LSI on the original matrix with high probability.


\begin{proof}
We have 
\[
A = \sum_{i=1}^n \sigma_i u_i v_i^T, \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,A_k = \sum_{i=1}^k \sigma_i u_i v_i^T,  \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, B=\sum_{i=1}^{\ell} \lambda_i a_i b_i^T,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, B_{2k} = A \sum_{i=1}^{2k} b_ib_i^T.
\]

%\[
% B=\sum_{i=1}^t \lambda_i a_i b_i^T\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, B_{2k} := A \sum_{i=1}^{2k} b_ib_i^T.
%\]

$b_1,...,b_n$ is an orthonormal set of vectors spanning the row space of $A$ (because $R$ is an orthonormal matrix, resulting in orthonormal projection) hence using the Parseval's identity we can write
\[
 \lVert A - B_{2k} \rVert_F^2 = \sum_{i=1}^n |(A - B_{2k})b_i|^2
\]
for $i = 1,...,2k$, because $b_i^Tb_i = 1$ we have
\[
(A - B_{2k})b_i = Ab_i - Ab_i = 0,
\]
and for  $i = 2k+1,...,n$, because $b_j^Tb_i = 0$ we have
\[
(A - B_{2k})b_i = Ab_i.
\]
Hence,

\begin{align*}
    \lVert A - B_{2k} \rVert_F^2 &= \sum_{i=1}^n |(A - B_{2k})b_i|^2 = \sum_{i=2k+1}^n |A b_i|^2 = \sum_{i=1}^n |A b_i|^2 - \sum_{i=1}^{2k} |A b_i|^2 \overset{\text{Parseval's id.}}{=} \lVert A \rVert_F^2 - \sum_{i=1}^{2k} |A b_i|^2 
\end{align*}

On the other hand, we have
\begin{align*}
 \lVert A - A_k \rVert_F^2 &\overset{\text{ \cref{lem:A_Ak}}}{=} \sum_{i=k+1}^n \sigma_i^2  \overset{\text{Frob. norm \cite{FrobeniusDef}}}{=}  \lVert A \rVert_F^2 - \lVert A_k \rVert_F^2. 
\end{align*}

Now we consider 
\begin{align*}
   \lVert A - B_{2k} \rVert_F^2 - \lVert A - A_k \rVert_F^2 &= \lVert A \rVert_F^2 - \sum_{i=1}^{2k} |A b_i|^2  - (\lVert A \rVert_F^2 - \lVert A_k \rVert_F^2) = \lVert A_k \rVert_F^2 - \sum_{i=1}^{2k} |A b_i|^2
\end{align*}
That is equivalent to
\begin{equation}
   \lVert A - B_{2k} \rVert_F^2 = \lVert A - A_k \rVert_F^2 + (\lVert A_k \rVert_F^2 - \sum_{i=1}^{2k} |A b_i|^2)  \label{eq:A} 
\end{equation}



For the next step, we show
\[
(1+\epsilon) \sum_{i=1}^{2k} |A b_i|^2 \geq \sum_{i=1}^{2k} \lambda_i^2.
\]
\begin{align*}
    \sum_{i=1}^{2k} \lambda_i^2 \overset{|Bb_i| = \lambda_i}{=}\sum_{i=1}^{2k}|Bb_i|^2 
    \overset{\text{sbst. B}}{=} \sum_{i=1}^{2k} \left| \sqrt{\dfrac{n}{\ell}} R^T (Ab_i)\right|^2
    = \sum_{i=1}^{2k} \dfrac{n}{\ell}\left| R^T (Ab_i)\right|^2
\end{align*}
%Random projection theorem
Now from the Johnson-Lindenstrauss lemma \cite{johnson1984Lindenstrauss} for very large $\ell \in \Omega((\log n)/\epsilon^2)$ we have for each $i$
\[
 \dfrac{n}{\ell}| R^T (Ab_i)|^2 \leq (1+\epsilon) |Ab_i|^2
\]
with high probability.

Hence with a high probability
\[
(1+\epsilon) \sum_{i=1}^{2k} |A b_i|^2 \geq \sum_{i=1}^{2k} \lambda_i^2.
\]

Now we have
\begin{align*}
    \sum_{i=1}^{2k} |A b_i|^2 \geq \dfrac{1}{(1+\epsilon)} \sum_{i=1}^{2k} \lambda_i^2 
    \overset{\text{Cor. \ref{cor:10}}}{\geq} \dfrac{(1-\epsilon)}{(1+\epsilon)} \lVert A_k \rVert_F^2 
    \geq (1-2\epsilon) \lVert A_k \rVert_F^2
\end{align*}
I.e.
\[
\sum_{i=1}^{2k} |A b_i|^2 \geq (1-2\epsilon) \lVert A_k \rVert_F^2
\]

Now we substitute this result in equation \ref{eq:A} getting
\begin{align*}
    \lVert A - B_{2k} \rVert_F^2 \leq \lVert A - A_k \rVert_F^2 + \lVert A_k \rVert_F^2 - (1-2\epsilon) \lVert A_k \rVert_F^2 
\iff \lVert A - B_{2k} \rVert_F^2 \leq \lVert A - A_k \rVert_F^2 + 2\epsilon\lVert A_k \rVert_F^2
\end{align*}

Due to the formulation of Frobenius norm as in Lemma \ref{lem:A_Ak}, %\cite{FrobeniusDef}
 we have  $\lVert A \rVert_F^2 \geq \lVert A_k \rVert_F^2$. This inequality leads us to the conclusive result of our proof
\[
\lVert A - B_{2k} \rVert_F^2 \leq \lVert A - A_k \rVert_F^2 + 2\epsilon\lVert A \rVert_F^2.
\]
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%REFORMULATE%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The two-step method offers significant computational savings compared to traditional Latent Semantic Indexing (LSI).
Consider a matrix \( A \) of size \( n \times m \). If \( A \) is sparse, containing approximately \( c \) nonzero entries per column (where \( c \) represents the average number of terms in a document), the time complexity to compute Latent Semantic Indexing (LSI) is \( O(mnc) \).
The time complexity for computing the random projection to \( \ell \) dimensions is \( O(mc\ell) \).
%%%%
After the projection, we compute LSI. Its computation is in $O(m\ell^2)$. Hence the total time is $O(m\ell(\ell+c))$.
For an $\epsilon$ approximation $\ell$ has to be in $O((\log n)/\epsilon^2)$. 
So, the two-step method outperforms the single-step method in terms of runtime. The time complexity is \(O(m(\log^2 n + c \log n))\) for the two-step method, whereas it is \(O(mnc)\) for the single-step method.








%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%






%We prove theorem 5 of the paper which maintains that the matrix obtained via random projection followed by LSI recovers almost as
%much as the matrix obtained by direct LSI, with high probability.



\section{Summary and Conclusion} %Kes EJic 
In our seminar paper, we gave the most relevant information of the scientific paper from Papadimitriou et al. \cite{APAPADIMITRIOU2000217} introducing the theorem \ref{theorem: lsiperform} which shows the LSI's good performance assuming some constraints on the given term-document matrix. These assumptions are sometimes not given in a real-world scenario. Hence the authors hope that this work will serve as a motivation for further work where these constraints on the term-document matrix are not given.

Furthermore, we considered the LSI by random projection a two-step LSI which reduces the computational time. We showed the central theorem \ref{theorem:imp} which argued that the two-step LSI is close to the single-step LSI with high probability also giving a detailed proof of the theorem.










\clearpage

\bibliographystyle{plainurl}
\bibliography{references.bib}





\end{document}





